{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(directory, label=None):\n",
    "    data_frames = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            if label is not None:\n",
    "                df[\"Label\"] = label  # Assign class labels\n",
    "            data_frames.append(df)\n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # âœ… Handle categorical data\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    # âœ… Handle missing values\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dir = \"dataset_grouped_47_healthy_cleaned\"\n",
    "mdd_dir = \"dataset_grouped_70_mdd_cleaned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_healthy = load_and_preprocess(healthy_dir, label=0)  # 0 for Healthy\n",
    "df_mdd = load_and_preprocess(mdd_dir, label=1)  # 1 for MDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flattened = X.reshape(X.shape[0], -1)  # (samples, features)\n",
    "\n",
    "# âœ… Apply StandardScaler\n",
    "X_scaled = scaler.fit_transform(X_flattened)\n",
    "\n",
    "# âœ… Reshape back to ensure correct dimensions\n",
    "X = X_scaled  # Keeps it 2D for ML models\n",
    "\n",
    "print(f\"âœ… Data Shape after Standardization: {X.shape}\")  # Should be (samples, features)\n",
    "\n",
    "# ================================\n",
    "# ðŸ”¹ Split into Train, Validation & Test\n",
    "# ================================\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # Reserve 20% for validation & test  \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.3,  # 30% of temp set â†’ 6% of total data\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# âœ… Convert to NumPy (if not already) and ensure correct dtype\n",
    "X_train_np = np.array(X_train, dtype=np.float32)\n",
    "y_train_np = np.array(y_train, dtype=np.float32)\n",
    "X_val_np = np.array(X_val, dtype=np.float32)\n",
    "\n",
    "# âœ… XGBoost Model with GPU Optimization\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",  # âœ… Correct method for GPU in XGBoost 2.0+\n",
    "    device=\"cuda\",  # âœ… Explicitly set device to GPU\n",
    "    eval_metric=\"logloss\",\n",
    "    learning_rate=0.005,\n",
    "    max_depth=15,\n",
    "    gamma=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1.5,\n",
    "    n_estimators=2000,  # âœ… Lowered for efficiency\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# âœ… Define Parameter Grid (For Fine-Tuning)\n",
    "param_grid = {\n",
    "    'n_estimators': [1500, 2000],         \n",
    "    'learning_rate': [0.005],        \n",
    "    'max_depth': [10, 15],              \n",
    "    'gamma': [0.1, 0.2],                 \n",
    "    'subsample': [0.8],            \n",
    "    'colsample_bytree': [0.8],     \n",
    "    'reg_alpha': [0.3],             \n",
    "    'reg_lambda': [1.5],            \n",
    "    'min_child_weight': [2]         \n",
    "}\n",
    "\n",
    "# âœ… Use StratifiedKFold for Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# âœ… GridSearchCV for Hyperparameter Tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv,  # âœ… Use StratifiedKFold\n",
    "    verbose=3,  \n",
    "    n_jobs=1  # âœ… Ensures GPU execution inside XGBoost\n",
    ")\n",
    "\n",
    "# âœ… Train with GridSearchCV (Fully GPU-optimized)\n",
    "grid_search.fit(X_train_np, y_train_np)  # ðŸš€ Data will be loaded into GPU inside XGBoost\n",
    "\n",
    "# âœ… Best Hyperparameters\n",
    "print(\"ðŸ”¥ Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# âœ… Test on Validation Set\n",
    "best_model = grid_search.best_estimator_\n",
    "val_preds = best_model.predict(X_val_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# âœ… Function to Load and Preprocess Data\n",
    "def load_and_preprocess(directory, label=None):\n",
    "    data_frames = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            if label is not None:\n",
    "                df[\"Label\"] = label  # Assign class labels\n",
    "            data_frames.append(df)\n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # âœ… Handle categorical data\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    # âœ… Handle missing values\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# âœ… Load MDD and Healthy Data (Training)\n",
    "healthy_dir = \"/home/admincit/Desktop/Team_4/dataset_grouped_47/dataset_grouped_47_healthy_cleaned\"\n",
    "mdd_dir = \"/home/admincit/Desktop/Team_4/dataset_grouped_47/dataset_grouped_70_mdd_cleaned\"\n",
    "\n",
    "df_healthy = load_and_preprocess(healthy_dir, label=0)  # 0 for Healthy\n",
    "df_mdd = load_and_preprocess(mdd_dir, label=1)  # 1 for MDD\n",
    "\n",
    "# âœ… Combine into a Single Training Dataset\n",
    "df_train = pd.concat([df_healthy, df_mdd], ignore_index=True)\n",
    "\n",
    "# âœ… Separate Features and Labels\n",
    "X_train = df_train.drop(columns=[\"Label\"])\n",
    "y_train = df_train[\"Label\"]\n",
    "\n",
    "# âœ… Scale Data (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# âœ… Save the Scaler for Later Use\n",
    "joblib.dump(scaler, \"model_weights/binary_xgb_scaler.pkl\")\n",
    "\n",
    "# âœ… Define Cross-Validation Strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ðŸ”¥ **Train XGBoost Model with Best Hyperparameters**\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",  # Binary classification\n",
    "    tree_method=\"hist\",  # Optimized method\n",
    "    device=\"cuda\",  # Use GPU\n",
    "    eval_metric=\"logloss\",  # Binary log loss\n",
    "    learning_rate=0.005,  \n",
    "    max_depth=25,  \n",
    "    gamma=0.2,  \n",
    "    subsample=0.95,  \n",
    "    colsample_bytree=0.97,  \n",
    "    min_child_weight=1,  \n",
    "    reg_alpha=0.8,  \n",
    "    reg_lambda=3.0,  \n",
    "    n_estimators=6000,  # High number for better learning\n",
    "    verbosity=1  # Output progress\n",
    ")\n",
    "\n",
    "# âœ… Perform Stratified K-Fold Cross-Validation\n",
    "cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "# âœ… Train XGBoost Model on Full Data\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# âœ… Save Trained Model\n",
    "joblib.dump(xgb_model, \"model_weights/binary_xgb_model.pkl\")\n",
    "print(f\"ðŸ”¥ XGBoost Model Trained! Cross-Validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "# âœ… Load MDD and Healthy Data (Validation)\n",
    "val_healthy_dir = \"/home/admincit/Desktop/Team_4/dataset_grouped_47/dataset_grouped_47_healthy_cleaned\"\n",
    "val_mdd_dir = \"/home/admincit/Desktop/Team_4/dataset_grouped_47/dataset_grouped_70_mdd_cleaned\"\n",
    "\n",
    "df_val_healthy = load_and_preprocess(val_healthy_dir, label=0)\n",
    "df_val_mdd = load_and_preprocess(val_mdd_dir, label=1)\n",
    "\n",
    "# âœ… Combine Validation Data\n",
    "df_validation = pd.concat([df_val_healthy, df_val_mdd], ignore_index=True)\n",
    "\n",
    "# âœ… Prepare Validation Data\n",
    "X_val = df_validation.drop(columns=[\"Label\"])\n",
    "y_val = df_validation[\"Label\"]\n",
    "\n",
    "# âœ… Apply Same Scaling as Training\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# âœ… Make Predictions on Validation Data\n",
    "val_preds = xgb_model.predict(X_val_scaled)\n",
    "val_probs = xgb_model.predict_proba(X_val_scaled)[:, 1]  # Probabilities for ROC AUC\n",
    "\n",
    "# âœ… Evaluate Performance\n",
    "accuracy = accuracy_score(y_val, val_preds)\n",
    "roc_auc = roc_auc_score(y_val, val_probs)\n",
    "report = classification_report(y_val, val_preds)\n",
    "\n",
    "print(f\"ðŸ”¥ Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ”¥ ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nðŸ”¹ Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
